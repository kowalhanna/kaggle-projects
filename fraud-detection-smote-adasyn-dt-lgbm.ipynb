{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Detection Project \n\n### ADASYN and SMOTE oversampling techniques, Decision Trees and LightGBM, SHAP values\n\n## Introduction\n\nThis project aims to address the issue of fraud detection in credit card transactions. I explored the effectiveness of different oversampling techniques, (ADASYN and SMOTE), combined with Decision Trees and LightGBM models. Additionally, I examine the interpretability of these models using decision tree plots, feature importances and SHAP (SHapley Additive exPlanations) values. \n\n## Dataset Information\n\nThe dataset contains transactions made by European cardholders in September 2013. It consists of transactions that occurred over two days, with a total of 284,807 transactions, out of which only 492 are fraudulent. Consequently, the dataset is highly unbalanced, with frauds accounting for only 0.172% of all transactions, which needs to be accounted.\n\nThe dataset consists of numerical input variables resulting from a Principal Component Analysis (PCA) transformation. The PCA-transformed features are labeled as V1, V2, (...), V28, while the 'Time' and 'Amount' features are not transformed. The 'Time' feature represents the seconds elapsed between each transaction and the first transaction in the dataset, while the 'Amount' feature denotes the transaction amount.\n\nThe response variable, 'Class', takes a value of 1 in case of fraud and 0 otherwise.\n\nDataset can be accessed here: [here](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data).\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#Import neccessary libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report, accuracy_score\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.tree import plot_tree\nimport shap\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set seed\nnp.random.seed(42)\n#Read in the dataset\ndf = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\ndf.drop(columns='Time', axis=1, inplace=True)\n\n#Set color palette\nsns.set_palette('seismic')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:13:04.638962Z","iopub.execute_input":"2024-05-08T15:13:04.639977Z","iopub.status.idle":"2024-05-08T15:13:07.861588Z","shell.execute_reply.started":"2024-05-08T15:13:04.639920Z","shell.execute_reply":"2024-05-08T15:13:07.860308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Look at the dataset summary and drop any duplicated rows\nprint(df.shape)\nprint(df.head())\nprint(df.info())\nprint(df.describe())\nprint(df.median())\ndf.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:13:07.864048Z","iopub.execute_input":"2024-05-08T15:13:07.865235Z","iopub.status.idle":"2024-05-08T15:13:09.490718Z","shell.execute_reply.started":"2024-05-08T15:13:07.865184Z","shell.execute_reply":"2024-05-08T15:13:09.489541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"As mentioned previously, the dataset contains very few fradulent transactions, which poses a challange, as the dataset is highly unbalanced.","metadata":{}},{"cell_type":"code","source":"print(df['Class'].value_counts())\nplt.figure(figsize=(8, 6))\ndf['Class'].value_counts().plot(kind='bar')\nplt.title('Distribution of Class')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:13:09.492055Z","iopub.execute_input":"2024-05-08T15:13:09.492440Z","iopub.status.idle":"2024-05-08T15:13:09.776626Z","shell.execute_reply.started":"2024-05-08T15:13:09.492409Z","shell.execute_reply":"2024-05-08T15:13:09.775304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distributions of features","metadata":{}},{"cell_type":"code","source":"# Function for plotting histograms of numerical columns in a DataFrame\ndef plot_hist(df, color='navy'):\n    '''\n    Plot Histogram for Numerical Features in a DataFrame\n    \n    Filters numerical columns from the provided DataFrame and plots their histograms for exploratory data analysis.\n    \n    '''\n    # Select numerical columns from the DataFrame\n    num_columns = df.select_dtypes(include='number').columns\n    \n    # Check if there are any numerical columns\n    if len(num_columns) == 0:\n        print(\"No numerical columns found in the DataFrame.\")\n        return\n    \n    # Calculate the number of rows needed for subplots\n    number_columns = len(num_columns)\n    number_rows = (number_columns + 1) // 2\n    \n    # Create subplots\n    fig, axes = plt.subplots(number_rows, 2, figsize=(12, 6*number_rows))\n    axes = axes.flatten()\n    plt.subplots_adjust(hspace=0.5)\n    \n    # Loop through numerical columns and plot histogram\n    for i, col in enumerate(num_columns):\n        ax = axes[i]\n        sns.histplot(data=df, x=col, color=color, ax=ax)\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel(None)\n        ax.set_ylabel('Count')\n    \n    # Remove excess subplot if the number of columns is odd\n    if number_columns % 2 != 0:\n        fig.delaxes(axes[-1])\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:13:09.778426Z","iopub.execute_input":"2024-05-08T15:13:09.778904Z","iopub.status.idle":"2024-05-08T15:13:09.790344Z","shell.execute_reply.started":"2024-05-08T15:13:09.778863Z","shell.execute_reply":"2024-05-08T15:13:09.789024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_hist(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:14:18.343439Z","iopub.execute_input":"2024-05-08T15:14:18.343903Z","iopub.status.idle":"2024-05-08T15:16:54.084581Z","shell.execute_reply.started":"2024-05-08T15:14:18.343871Z","shell.execute_reply":"2024-05-08T15:16:54.083437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the IQR of columns\nfor column in df.columns:\n    print(f\"Checking {column}...\")\n    print()\n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    print(f\"IQR: {iqr}\")\n\n    # Identify lower outliers \n    lower_outliers = df[df[column] < q1 - 1.5 * iqr]\n    upper_outliers = df[df[column] > q3 + 1.5 * iqr]\n\n    # Calculate the number of lower outliers and upper outliers\n    num_lower_outliers = len(lower_outliers)\n    num_upper_outliers = len(upper_outliers)\n    num_total_outliers = num_lower_outliers + num_upper_outliers\n    print(f\"Number of lower outliers: {num_lower_outliers}\")\n    print(f\"Number of upper outliers: {num_upper_outliers}\")\n    print(f\"Total number of outliers: {num_total_outliers}\")\n\n    # Calculate proportion of outliers in the dataset\n    proportion_outliers = num_total_outliers / len(df) * 100\n    print(f\"Proportion of outliers in the dataset: {proportion_outliers:.2f}%\")\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:18.765346Z","iopub.execute_input":"2024-05-06T14:48:18.765789Z","iopub.status.idle":"2024-05-06T14:48:19.298882Z","shell.execute_reply.started":"2024-05-06T14:48:18.765751Z","shell.execute_reply":"2024-05-06T14:48:19.297636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After analyzing the outliers identified through the IQR method, it's evident that certain columns exhibit a notable number of outliers. It's essential to consider these outliers when deciding on transformations for the data.","metadata":{}},{"cell_type":"code","source":"# Check skew of the features\ndf.skew().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:19.301535Z","iopub.execute_input":"2024-05-06T14:48:19.302002Z","iopub.status.idle":"2024-05-06T14:48:19.390788Z","shell.execute_reply.started":"2024-05-06T14:48:19.301960Z","shell.execute_reply":"2024-05-06T14:48:19.389575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check correlations with target variable\ncorrelations = df.corr()\nclass_correlations = correlations['Class'].sort_values(ascending=False)\nprint(class_correlations)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:19.392627Z","iopub.execute_input":"2024-05-06T14:48:19.393084Z","iopub.status.idle":"2024-05-06T14:48:20.120565Z","shell.execute_reply.started":"2024-05-06T14:48:19.393046Z","shell.execute_reply":"2024-05-06T14:48:20.119320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE and ADASYN oversampling\n\nIn order to mitigate the imbalance in the dataset, two oversampling techniques were employed:\n\n- **SMOTE (Synthetic Minority Over-sampling Technique)** generates synthetic samples for the minority class by interpolating between existing minority class samples, balancing class distribution.\n- **ADASYN (Adaptive Synthetic Sampling)** is an extension of SMOTE that applies higher sampling intensity to minority class instances that are more difficult to learn, adapting to the dataset's complexity.\n\nWe will assess the performance of each technique to determine its effectiveness for this specific task.","metadata":{}},{"cell_type":"code","source":"# Split into train and test sets\nX = df.drop('Class', axis=1)  \ny = df['Class'] \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify= y, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:20.122108Z","iopub.execute_input":"2024-05-06T14:48:20.122540Z","iopub.status.idle":"2024-05-06T14:48:20.333508Z","shell.execute_reply.started":"2024-05-06T14:48:20.122490Z","shell.execute_reply":"2024-05-06T14:48:20.332432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use SMOTE and ADASYN oversampling on training set\nsmote = SMOTE(random_state=42)\nadasyn = ADASYN(random_state=42)\nX_smote, y_smote = smote.fit_resample(X_train, y_train)\nX_adasyn, y_adasyn = adasyn.fit_resample(X_train, y_train)\n\nprint(y_smote.value_counts())\nprint(y_adasyn.value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:20.337466Z","iopub.execute_input":"2024-05-06T14:48:20.337821Z","iopub.status.idle":"2024-05-06T14:48:21.501540Z","shell.execute_reply.started":"2024-05-06T14:48:20.337793Z","shell.execute_reply":"2024-05-06T14:48:21.500048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model selection\n\nFor both training sets, model selection with 5-fold cross-validation was employed. Candidates included Logistic Regression, Decision Trees, and LightGBM. Decision Trees and LightGBM exhibited similar performance, with Logistic Regression slightly lagging behind.\n\nDecision Trees and LightGBM were selected for further tuning.","metadata":{}},{"cell_type":"code","source":"# Function for model selection\n\ndef model_selection(models, scoring_metric, X_train=X_train, y_train=y_train, n_splits=5):\n    '''\n    This function performs model selection by evaluating the performance of different models using cross-validation.\n    \n    Parameters:\n    - models (dict): A dictionary containing model names as keys and corresponding model instances as values.\n    - scoring_metric (str): The evaluation metric to use for comparing models.\n    - X_train (array-like): The feature matrix of the training data.\n    - y_train (array-like): The target vector of the training data.\n    \n    Returns:\n    - results (dict): A dictionary containing model names as keys and the corresponding cross-validation results as values.\n    '''\n    results = {}\n    \n    for model_name, model in models.items():\n        kf = KFold(n_splits=n_splits, random_state=12, shuffle=True)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kf, scoring=scoring_metric)\n        results[model_name] = cv_results\n\n    plt.figure(figsize=(10, 6))\n    plt.boxplot(results.values(), labels=results.keys())\n    plt.title('Model Performance Comparison')\n    plt.xlabel('Model')\n    plt.ylabel(scoring_metric)\n    plt.grid(True)\n    plt.show()\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:21.502817Z","iopub.execute_input":"2024-05-06T14:48:21.503155Z","iopub.status.idle":"2024-05-06T14:48:21.516056Z","shell.execute_reply.started":"2024-05-06T14:48:21.503127Z","shell.execute_reply":"2024-05-06T14:48:21.514795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the models and scoring metric\nmodels = {\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'LightGBM': LGBMClassifier(random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n}\n\nscoring_metric = 'roc_auc'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:21.517761Z","iopub.execute_input":"2024-05-06T14:48:21.518277Z","iopub.status.idle":"2024-05-06T14:48:21.530469Z","shell.execute_reply.started":"2024-05-06T14:48:21.518238Z","shell.execute_reply":"2024-05-06T14:48:21.529082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_selection(models=models, scoring_metric=scoring_metric, X_train=X_smote, y_train=y_smote)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:48:21.532085Z","iopub.execute_input":"2024-05-06T14:48:21.532542Z","iopub.status.idle":"2024-05-06T14:52:21.836880Z","shell.execute_reply.started":"2024-05-06T14:48:21.532490Z","shell.execute_reply":"2024-05-06T14:52:21.835623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_selection(models=models, scoring_metric=scoring_metric, X_train=X_adasyn, y_train=y_adasyn)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:52:21.838737Z","iopub.execute_input":"2024-05-06T14:52:21.839101Z","iopub.status.idle":"2024-05-06T14:56:46.492005Z","shell.execute_reply.started":"2024-05-06T14:52:21.839070Z","shell.execute_reply":"2024-05-06T14:56:46.490613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning and Model Training\n\nBoth Decision Tree Classifier and LightGBM Classifier were further tuned on both training sets. Quantile Transformer was used as a preprocessor to mitigate the skewed feature distributions.","metadata":{}},{"cell_type":"code","source":"# Define preprocessor and pipeline for LightGBM\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('quantile', QuantileTransformer(output_distribution='normal'), slice(0,None)),\n    ],\n    remainder='passthrough'\n)\n\npipeline_lgb = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LGBMClassifier(verbosity=-1))  \n])\n\nlgb_param_grid = {\n    'classifier__n_estimators': [400],  \n    'classifier__max_depth': [10], \n    'classifier__min_child_samples': [20, 30],\n    'classifier__learning_rate': [0.05],\n    'classifier__colsample_bytree': [0.8, 0.9]\n}\n\n# Perform hyperparameter tuning for both sets\nrandom_search_lgb = RandomizedSearchCV(pipeline_lgb, param_distributions=lgb_param_grid, n_iter=20, cv=5, scoring='roc_auc', random_state=42, n_jobs=-1)\n\nbest_models = {}\n\nfor X, y, sampler_name in zip([X_smote, X_adasyn], [y_smote, y_adasyn], ['SMOTE', 'ADASYN']):\n    random_search_lgb.fit(X, y)\n    best_models[sampler_name] = {\n        'best_params': random_search_lgb.best_params_,\n        'best_estimator': random_search_lgb.best_estimator_\n    }\n    print(f\"Best Hyperparameters for {sampler_name}: {random_search_lgb.best_params_}\")\n    print()\n\nbest_model_smote_lgb = best_models['SMOTE']['best_estimator']\nbest_model_adasyn_lgb = best_models['ADASYN']['best_estimator']\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:56:46.493819Z","iopub.execute_input":"2024-05-06T14:56:46.494205Z","iopub.status.idle":"2024-05-06T15:33:26.423056Z","shell.execute_reply.started":"2024-05-06T14:56:46.494176Z","shell.execute_reply":"2024-05-06T15:33:26.421714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define preprocessor and pipeline for DT\n\npipeline_dt = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', DecisionTreeClassifier())  \n])\n\ndt_param_grid = {\n    'classifier__max_depth': [10], \n    'classifier__min_samples_split': [20, 30],\n    'classifier__min_samples_leaf': [5, 10]\n}\n\n# Perform hyperparameter tuning for both sets\nrandom_search_dt = RandomizedSearchCV(pipeline_dt, param_distributions=dt_param_grid, n_iter=30, cv=5, scoring='roc_auc', random_state=42, n_jobs=-1)\n\nbest_models_dt = {}\n\nfor X, y, sampler_name in zip([X_smote, X_adasyn], [y_smote, y_adasyn], ['SMOTE', 'ADASYN']):\n    random_search_dt.fit(X, y)\n    best_models_dt[sampler_name] = {\n        'best_params': random_search_dt.best_params_,\n        'best_estimator': random_search_dt.best_estimator_\n    }\n    print(f\"Best Hyperparameters for {sampler_name}: {random_search_dt.best_params_}\")\n    print()\n    \nbest_model_smote_dt = best_models['SMOTE']['best_estimator']\nbest_model_adasyn_dt = best_models['ADASYN']['best_estimator']","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:33:26.424621Z","iopub.execute_input":"2024-05-06T15:33:26.425594Z","iopub.status.idle":"2024-05-06T15:46:45.718640Z","shell.execute_reply.started":"2024-05-06T15:33:26.425561Z","shell.execute_reply":"2024-05-06T15:46:45.717284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\n\nUpon training and tuning, all four models were evaluated. Both SMOTE and ADASYN training sets exhibited similar performance. However in the end, SMOTE demonstrated slightly superior performance in critical metrics - TPR and FNR, as well precision and recall for the positive class (our 'fraud')\n\nIt is impotrant to note however, that with different hyperparameters during training, there were instances when ADASYN set performed slightly better, so the diffrence is not that notable.\n\nInterestingly, both LightGBM and Decision Trees performed very similarly in this task.","metadata":{}},{"cell_type":"code","source":"# Evaluate all four models\nmodels = {\n    'LGBM (SMOTE)': best_model_smote_lgb,\n    'LGBM (ADASYN)': best_model_adasyn_lgb,\n    'Decision Tree (SMOTE)': best_model_smote_dt,\n    'Decision Tree (ADASYN)': best_model_adasyn_dt\n}\n\nfor model_name, model in models.items():\n    \n    y_pred = model.predict(X_test)\n    \n    # Classification Report\n    print(f\"Classification report for {model_name}:\")\n    print(classification_report(y_test, y_pred))\n    \n    # ROC AUC score\n    print(f\"ROC AUC for {model_name}:\")\n    print(roc_auc_score(y_test, y_pred))\n    \n    # TPR and FNR\n    \n    conf_matrix = confusion_matrix(y_test, y_pred)\n    TN, FP, FN, TP = conf_matrix.ravel()\n    TPR = TP / (TP + FN)\n    FNR = FN / (TP + FN)\n    print(f\"True Positive Rate for {model_name}:\", TPR)\n    print(f\"False Negative Rate for {model_name}:\", FNR)\n    print()\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:46:45.720574Z","iopub.execute_input":"2024-05-06T15:46:45.721301Z","iopub.status.idle":"2024-05-06T15:47:00.356214Z","shell.execute_reply.started":"2024-05-06T15:46:45.721257Z","shell.execute_reply":"2024-05-06T15:47:00.354922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Interpretation - Decision Tree plots, Feature Importances, SHAP values\n\nLet's look at tree plots, feature importances and a summary plot of SHAP values to further understand which features proved to be crucial for the models.","metadata":{}},{"cell_type":"code","source":"# Plot Decision Trees\nfor sampler_name, best_model_info in best_models_dt.items():\n    best_estimator = best_model_info['best_estimator']\n    classifier = best_estimator.named_steps['classifier']  \n\n    if isinstance(classifier, DecisionTreeClassifier):\n        plt.figure(figsize=(50, 20))\n        plot_tree(classifier, feature_names=feature_names, class_names=class_names, filled=True, max_depth=3)\n        plt.title(f'Decision Tree for {sampler_name} set')\n        plt.show()\n    else:\n        print(f\"The classifier for {sampler_name} is not a Decision Tree Classifier.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:47:00.358032Z","iopub.execute_input":"2024-05-06T15:47:00.358573Z","iopub.status.idle":"2024-05-06T15:47:04.092029Z","shell.execute_reply.started":"2024-05-06T15:47:00.358541Z","shell.execute_reply":"2024-05-06T15:47:04.091098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon looking at the first three layers of the tree plots, we observe that V14 has a position as the root node and is the most crucial feature in the decision-making process for both models. Furthermore, features such as V4, Amount, and V12 are close to the root node across these layers, suggesting their importance in the decision paths of the tree.","metadata":{}},{"cell_type":"code","source":"# Plot feature importances for SMOTE and ADASYN for LGBM\n\nsorted_indices_smote = np.argsort(feature_importance_smote)\nsorted_indices_adasyn = np.argsort(feature_importance_adasyn)\n\nsorted_feature_names_smote = np.array(feature_names)[sorted_indices_smote]\nsorted_feature_importance_smote = feature_importance_smote[sorted_indices_smote]\n\nsorted_feature_names_adasyn = np.array(feature_names)[sorted_indices_adasyn]\nsorted_feature_importance_adasyn = feature_importance_adasyn[sorted_indices_adasyn]\n\nplt.figure(figsize=(10, 6))\nplt.barh(sorted_feature_names_smote, sorted_feature_importance_smote)\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.title('Feature Importances for SMOTE set')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.barh(sorted_feature_names_adasyn, sorted_feature_importance_adasyn)\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.title('Feature Importances for ADASYN set')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:47:04.093408Z","iopub.execute_input":"2024-05-06T15:47:04.094326Z","iopub.status.idle":"2024-05-06T15:47:04.954481Z","shell.execute_reply.started":"2024-05-06T15:47:04.094295Z","shell.execute_reply":"2024-05-06T15:47:04.953187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The feature importance plots for LightGBM models suggest that Amount had by far biggest importance for both training sets, followed by V14 and V4 consistently. After that, there are slight differences between the two LGBM models. Let us also look at SHAP values summary plot for the SMOTE and ADASYN datasets.","metadata":{}},{"cell_type":"code","source":"# Compute SHAP values for the SMOTE training set and plot summary plot\nshap.initjs()\nquant = QuantileTransformer(output_distribution='normal')\nX_smote_transformed = quant.fit_transform(X_smote)\nX_test_transformed = quant.transform(X_test)\n\nlgb = LGBMClassifier(min_samples_split=20, min_samples_leaf=5, max_depth=10, random_state=42, verbosity=-1)\nlgb.fit(X_smote_transformed, y_smote)\n\nexplainer = shap.TreeExplainer(lgb)\nshap_values = explainer.shap_values(X_test_transformed)\nshap.summary_plot(shap_values, X_test_transformed, feature_names=X_test.columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:47:04.956102Z","iopub.execute_input":"2024-05-06T15:47:04.956588Z","iopub.status.idle":"2024-05-06T15:48:04.453477Z","shell.execute_reply.started":"2024-05-06T15:47:04.956540Z","shell.execute_reply":"2024-05-06T15:48:04.452210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the SHAP values are quite different from the feature importances, which stems from differences between those metrics. Hee, the mean SHAP value for Amount is notably lower compared to its prominent position in the feature importances plot. Instead, features like V10 and V14 exhibit the highest impact on the model output, followed by V1 and V4.","metadata":{}},{"cell_type":"code","source":"# SHAP summary plot for ADASYN set\nX_adasyn_transformed = quant.fit_transform(X_adasyn)\nX_test_transformed = quant.transform(X_test)\n\nlgb2 = LGBMClassifier(min_samples_split=20, min_samples_leaf=5, max_depth=10, random_state=42, verbosity=-1)\nlgb2.fit(X_adasyn_transformed, y_adasyn)\n\nexplainer = shap.TreeExplainer(lgb2)\nshap_values = explainer.shap_values(X_test_transformed)\nshap.summary_plot(shap_values, X_test_transformed, feature_names=X_test.columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:48:04.455776Z","iopub.execute_input":"2024-05-06T15:48:04.456259Z","iopub.status.idle":"2024-05-06T15:49:03.354067Z","shell.execute_reply.started":"2024-05-06T15:48:04.456208Z","shell.execute_reply":"2024-05-06T15:49:03.352454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second model made with ADASYN training set, however it has very similar performance, it has also quite different SHAP values. For instance, while V14 remains a significant feature (ranking first in importance), V10 is ranked sixth. This observation underscores that while there are similarities in feature importance across various models, we can see quite distinct differences too.\n","metadata":{}}]}